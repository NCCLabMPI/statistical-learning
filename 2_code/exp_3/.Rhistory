all.list  <- list()
all.cor.list  <- list()
all.cor.z.list  <- list()
all.cor.diss.z.list <- list()
all.cor.diss.list <- list()
for (curr_subj in unique(exps.3.4.data$subject)) {
curr_data <- exps.3.4.data %>%
filter(subject == curr_subj) %>%
dplyr::select(target,rt) %>%
na.omit() %>%
group_by(target) %>%
dplyr::mutate(row_id=1:n()) %>%
ungroup() %>%
spread(key = target, value = rt) %>%
#ungroup(subject)
dplyr::select(-row_id)
# find minimum number of rows, N
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
# exceptions
if(curr_subj == "s1107") { # 3/26
# subject had only 2 entires in tu, so just make the column NA to preserve other data
curr_data <- curr_data %>%
mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2] # select the second lowest length
} else if (curr_subj == "l2805") { # 3/21
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2]
} else if (curr_subj == "h0207") { # 3/11
curr_data <- curr_data %>%
add_column(ro = as.numeric(rep(NA,nrow(curr_data))))
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[2]
} else if (curr_subj == "g0609") { # 3/8
curr_data <- curr_data %>%
mutate(po = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2] # select the second lowest length
# and maybe also ga?
} else if (curr_subj == "b0207") { # 3/2
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
add_column(ro = as.numeric(rep(NA,nrow(curr_data))),.after = "di")
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[3]
} else if (curr_subj == "a0604") { # 3/1
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2]
} else if (curr_subj == "s2205") { # 4/16
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
add_column(se = as.numeric(rep(NA,nrow(curr_data))),.after = "ki") %>%
mutate(za = as.numeric(rep(NA,nrow(curr_data)))) %>%
mutate(ga = as.numeric(rep(NA,nrow(curr_data))))
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[5]
} else if (curr_subj == "s0310") { # 4/12
# subject is missing be data, so just make it NA, N is the same
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za")
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[2]
} else if (curr_subj == "a2605") { # 4/2
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data)))) %>%
mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[3]
} else {
N <- min(n_rows)
}
# create output matrix with N rows
curr_data.mat <- curr_data[1:N,]
# shuffle longer columns into shorter new matrix
for (i in 1:length(curr_data.mat)) {
if (n_rows[i] < N) {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:N,i]), size = N)
} else {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:n_rows[i],i]), size = N)}
}
# take correlation(), then transform to z values()
curr_data.cor <- cor(curr_data.mat) # corr
curr_data.cor.z <- FisherZ(cor(curr_data.mat)) # corr - fisher
curr_data.cor.diss.z <- FisherZ(1-cor(curr_data.mat)) # diss - fisher
curr_data.cor.diss <- 1-(cor(curr_data.mat)) # diss
# append
all.list <- append(all.list, list(curr_data.mat))
all.cor.list <- append(all.cor.list, list(curr_data.cor))
all.cor.z.list <- append(all.cor.z.list, list(curr_data.cor.z))
all.cor.diss.z.list <- append(all.cor.diss.z.list, list(curr_data.cor.diss.z))
all.cor.diss.list <- append(all.cor.diss, list(curr_data.cor.diss))
}
# For each subject, compute a correlation matrix between syllables and then z transform that data. Put matrices into a list, and average them. Then transform back to rho.
all.list  <- list()
all.cor.list  <- list()
all.cor.z.list  <- list()
all.cor.diss.z.list <- list()
all.cor.diss.list <- list()
for (curr_subj in unique(exps.3.4.data$subject)) {
curr_data <- exps.3.4.data %>%
filter(subject == curr_subj) %>%
dplyr::select(target,rt) %>%
na.omit() %>%
group_by(target) %>%
dplyr::mutate(row_id=1:n()) %>%
ungroup() %>%
spread(key = target, value = rt) %>%
#ungroup(subject)
dplyr::select(-row_id)
# find minimum number of rows, N
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
# exceptions
if(curr_subj == "s1107") { # 3/26
# subject had only 2 entires in tu, so just make the column NA to preserve other data
curr_data <- curr_data %>%
mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2] # select the second lowest length
} else if (curr_subj == "l2805") { # 3/21
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2]
} else if (curr_subj == "h0207") { # 3/11
curr_data <- curr_data %>%
add_column(ro = as.numeric(rep(NA,nrow(curr_data))))
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[2]
} else if (curr_subj == "g0609") { # 3/8
curr_data <- curr_data %>%
mutate(po = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2] # select the second lowest length
# and maybe also ga?
} else if (curr_subj == "b0207") { # 3/2
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
add_column(ro = as.numeric(rep(NA,nrow(curr_data))),.after = "di")
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[3]
} else if (curr_subj == "a0604") { # 3/1
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2]
} else if (curr_subj == "s2205") { # 4/16
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
add_column(se = as.numeric(rep(NA,nrow(curr_data))),.after = "ki") %>%
mutate(za = as.numeric(rep(NA,nrow(curr_data)))) %>%
mutate(ga = as.numeric(rep(NA,nrow(curr_data))))
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[5]
} else if (curr_subj == "s0310") { # 4/12
# subject is missing be data, so just make it NA, N is the same
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za")
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[2]
} else if (curr_subj == "a2605") { # 4/2
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data)))) %>%
mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[3]
} else {
N <- min(n_rows)
}
# create output matrix with N rows
curr_data.mat <- curr_data[1:N,]
# shuffle longer columns into shorter new matrix
for (i in 1:length(curr_data.mat)) {
if (n_rows[i] < N) {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:N,i]), size = N)
} else {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:n_rows[i],i]), size = N)}
}
# take correlation(), then transform to z values()
curr_data.cor <- cor(curr_data.mat) # corr
curr_data.cor.z <- FisherZ(cor(curr_data.mat)) # corr - fisher
curr_data.cor.diss.z <- FisherZ(1-cor(curr_data.mat)) # diss - fisher
curr_data.cor.diss <- 1-(cor(curr_data.mat)) # diss
# append
all.list <- append(all.list, list(curr_data.mat))
all.cor.list <- append(all.cor.list, list(curr_data.cor))
all.cor.z.list <- append(all.cor.z.list, list(curr_data.cor.z))
all.cor.diss.z.list <- append(all.cor.diss.z.list, list(curr_data.cor.diss.z))
all.cor.diss.list <- append(all.cor.diss.list, list(curr_data.cor.diss))
}
# avg.z.list <- reduce(all.z.list, `+`, na.rm = TRUE) / length(all.z.list)
# ^ can't handle NAs
# avg.cor.list <- rowMeans(do.call(cbind, all.cor.list), na.rm = TRUE)
# ^ doesn't perform intended function (mean matrix), but also, the Inf values generated
# by the Fisher transform make the outputs Inf. Possible solution is to recode z of Inf
# for cor of 1 to the z value for a correlation of 0.99...
# But I wonder if it's necesary to perform the z transform. Why not just average the correlation
# matrices? After all, they are all on the same scale and express relationships between the items,
# rather than absolute values, which is what we intended.
# So now, just find how to perform matrix average that ignores NAs in the correlation matrix.
# Fisher Z merely makes the sampling distribution more normal to make calculation easier, but
# it's not required if the computation is performed on a computer that can handle the exact
# (however skewed) true distribution: https://stats.stackexchange.com/questions/420142/why-is-fisher-transformation-necessary
# Ok, these work! For the z's, the diag is infinite
# all.cor <- apply(simplify2array(all.cor.list), 1:2, mean, na.rm= TRUE)
# all.z <- apply(simplify2array(all.cor.z.list), 1:2, mean, na.rm= TRUE)
# The results of these two are nearly identical -- identical out to about 3 digits, but indeed the z's are normal-ish
# all.diss.z <- FisherZ(all.cor.diss)
# apply(simplify2array(all.cor.diss),1:2, na.rm = TRUE)
# indices for each matrix
# wn.TP <- list(c(1,4),c(1,7),c(1,10),c(4,7),c(4,10),c(7,10), # low TP
#               c(2,5),c(2,8),c(2,11),c(5,8),c(5,11),c(8,11), # high TP
#              c(3,6),c(3,9),c(3,12),c(6,9),c(6,12),c(9,12))
# ac.TP <- list(c(1,2),c(4,5),c(7,8),c(10,11)) # .33-1 crosses within words
wn.TP <- list(c(1,4),c(1,7),c(1,10),c(4,7),c(4,10),c(7,10)) # low TP
ac.TP <- list(c(2,5),c(2,8),c(2,11),c(5,8),c(5,11),c(8,11), # high TP
c(3,6),c(3,9),c(3,12),c(6,9),c(6,12),c(9,12))
wn.tp.arr <- list()
for (i in 1:length(wn.TP)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
wn.tp.arr <- append(wn.tp.arr, mapply(function(x, y) x[y][2], all.cor.list, wn.TP[i], SIMPLIFY = FALSE))
}
ac.tp.arr <- list()
for (i in 1:length(ac.TP)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
ac.tp.arr <- append(ac.tp.arr, mapply(function(x, y) x[y][2], all.cor.list, ac.TP[i], SIMPLIFY = FALSE))
}
within.TP <- cbind(wn.tp.arr[!is.na(wn.tp.arr)]) %>% as.numeric()
across.TP <- cbind(ac.tp.arr[!is.na(ac.tp.arr)]) %>% as.numeric()
n.tp <- round(min(length(within.TP),length(across.TP))*(4/5))
set.seed(42)
within.TP.samp <- replicate(200, sample(within.TP, size = n.tp, replace = TRUE))
within.TP.means <- replicate(200, mean(sample(within.TP, size = n.tp, replace = TRUE)))
across.TP.samp <- replicate(200, sample(across.TP, size = n.tp, replace = TRUE))
across.TP.means <- replicate(200, mean(sample(across.TP, size = n.tp, replace = TRUE)))
(wt.TP <- wilcox.test(within.TP.samp,across.TP.samp, conf.int = TRUE, conf.level = 0.95, paired = FALSE))
# indices for each matrix
wn.OP <- list(c(1,4),c(1,7),c(1,10),c(4,7),c(4,10),c(7,10), # 1s
c(2,5),c(2,8),c(2,11),c(5,8),c(5,11),c(8,11), #2s
c(3,6),c(3,9),c(3,12),c(6,9),c(6,12),c(9,12)) #3s
ac.OP <- list(c(1,2),c(2,3),c(1,3),
c(4,5),c(5,6),c(4,6),
c(7,8),c(7,9),c(8,9),
c(10,11),c(11,12),c(10,12))
# collect all possible values
wn.arr <- list()
for (i in 1:length(wn.OP)) {
wn.arr <- append(wn.arr, mapply(function(x, y) x[y][2], all.cor.list, wn.OP[i], SIMPLIFY = FALSE))
}
ac.arr <- list()
for (i in 1:length(ac.OP)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
ac.arr <- append(ac.arr, mapply(function(x, y) x[y][2], all.cor.list, ac.OP[i], SIMPLIFY = FALSE))
}
within.OP <- cbind(wn.arr[!is.na(wn.arr)]) %>% as.numeric()
across.OP <- cbind(ac.arr[!is.na(ac.arr)]) %>% as.numeric()
# compute maximum length
n.op <- round(min(length(within.OP),length(across.OP))*(4/5))
# sample & bootstrap to produce a matrix and bootstrapped means
set.seed(42)
within.OP.samp <- replicate(200, sample(within.OP, size = n.op, replace = TRUE))
within.OP.means <- replicate(200, mean(sample(within.OP, size = n.op, replace = TRUE)))
across.OP.samp <- replicate(200, sample(across.OP, size = n.op, replace = TRUE))
across.OP.means <- replicate(200, mean(sample(across.OP, size = n.op, replace = TRUE)))
(wt.OP <- wilcox.test(within.OP.samp,across.OP.samp, conf.int = TRUE, conf.level = 0.95, paired = FALSE))
wn.WI <- list(c(1,2),c(2,3),c(1,3),# nugadi
c(4,5),c(5,6),c(4,6),# rokise
c(7,8),c(7,9),c(8,9),# mipola
c(10,11),c(11,12),c(10,12))# zabetu
ac.WI <- list(c(1,5),c(2,6),c(1,6),c(1,8),c(2,9),c(1,9),c(1,11),c(2,12),c(1,12),# nu-ki, ga-se, nu-se, nu-po, ga-la, nu-la, nu-be, ga-tu, nu-tu
c(4,2),c(5,3),c(4,3),c(4,8),c(5,9),c(4,9),c(4,11),c(5,12),c(4,12), # ro-ga, ki-di, ro-di, ro-po, ki-la, ro-la, ro-be, ki-tu, ro-tu
c(7,2),c(8,3),c(7,3),c(7,5),c(8,6),c(7,6),c(7,11),c(8,12),c(7,12), # mi-ga, po-di, mi-di, mi-ki, po-se, mi-se, mi-be, po-tu, mi-tu
c(10,2),c(11,3),c(10,3),c(11,5),c(10,6),c(11,6),c(10,8),c(11,9),c(10,9)) # za-ga, be-di, za-di, za-ki, be-ki, za-se, za-po, be-la, za-la
# nope, these are all 3-1s
# list(c(3,4),c(3,7),c(3,10), #di-ro, di-mi, di-za
#             c(6,1),c(6,7),c(6,10), #se-nu, se-mi, se-za
#             c(9,1),c(9,4),c(9,10), #la-nu, la-ro, la-za
#             c(12,1),c(12,4,c(12,7))) #za-nu, za-ro, za-mi
wn.WI.arr <- list()
for (i in 1:length(wn.WI)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
wn.WI.arr <- append(wn.WI.arr, mapply(function(x, y) x[y][2], all.cor.list, wn.WI[i], SIMPLIFY = FALSE))
}
ac.WI.arr <- list()
for (i in 1:length(ac.WI)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
ac.WI.arr <- append(ac.WI.arr, mapply(function(x, y) x[y][2], all.cor.list, ac.WI[i], SIMPLIFY = FALSE))
}
within.WI <- cbind(wn.WI.arr[!is.na(wn.WI.arr)]) %>% as.numeric()
across.WI <- cbind(ac.WI.arr[!is.na(ac.WI.arr)]) %>% as.numeric()
n.wi <- round(min(length(within.WI),length(across.WI))*(4/5))
set.seed(42)
within.WI.samp <- replicate(200, sample(within.WI, size = n.wi, replace = TRUE))
within.WI.means <- replicate(200, mean(sample(within.WI, size = n.wi, replace = TRUE)))
across.WI.samp <- replicate(200, sample(across.WI, size = n.wi, replace = TRUE))
across.WI.means <- replicate(200, mean(sample(across.WI, size = n.wi, replace = TRUE)))
(wt.WI <- wilcox.test(within.WI.samp,across.WI.samp, conf.int = TRUE, conf.level = 0.95, paired = FALSE))
wn.di <- list(c(1,2),c(2,3),
c(4,5),c(5,6),
c(7,8),c(7,9),
c(10,11),c(11,12))
ac.di <- list(c(1,3),c(4,6),c(8,9),c(10,12))
wn.di.arr <- list()
for (i in 1:length(wn.di)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
wn.di.arr <- append(wn.di.arr, mapply(function(x, y) x[y][2], all.cor.list, wn.di[i], SIMPLIFY = FALSE))
}
ac.di.arr <- list()
for (i in 1:length(ac.di)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
ac.di.arr <- append(ac.di.arr, mapply(function(x, y) x[y][2], all.cor.list, ac.di[i], SIMPLIFY = FALSE))
}
within.di <- cbind(wn.di.arr[!is.na(wn.di.arr)]) %>% as.numeric()
across.di <- cbind(ac.di.arr[!is.na(ac.di.arr)]) %>% as.numeric()
n.di <- round(min(length(within.di),length(across.di))*(4/5))
set.seed(42)
within.di.samp <- replicate(200, sample(within.di, size = n.di, replace = TRUE))
within.di.means <- replicate(200, mean(sample(within.di, size = n.di, replace = TRUE)))
across.di.samp <- replicate(200, sample(across.di, size = n.di, replace = TRUE))
across.di.means <- replicate(200, mean(sample(across.di, size = n.di, replace = TRUE)))
(wt.DI <- wilcox.test(within.di.samp,across.di.samp, conf.int = TRUE, conf.level = 0.95, paired = FALSE))
WA.wilcox <- data.frame(test = factor(c(
"ordinal position",
#"ordinal position",
"transitional probability",
#"transitional probability",
"word identity",
#"word identity",
#"duplets",
"duplets"), levels = c("ordinal position","transitional probability","word identity","duplets")),
#group = as.factor(c("within","across","within","across","within","across","within","across")),
mean = as.numeric(c(mean(within.OP.means)-mean(across.OP.means),
mean(within.TP.means)-mean(across.TP.means),
mean(within.WI.means)-mean(across.WI.means),
mean(within.di.means)-mean(across.di.means))),
w = as.numeric(c(
wt.OP$statistic,
#wt.OP$statistic,
wt.TP$statistic,
#wt.TP$statistic,
wt.WI$statistic,
#wt.WI$statistic,
#wt.DI$statistic,
wt.DI$statistic)),
"CI low" = as.numeric(c(
wt.OP$conf.int[1],
#wt.OP$conf.int,
wt.TP$conf.int[1],
#wt.TP$conf.int,
wt.WI$conf.int[1],
#wt.WI$conf.int,
#wt.DI$conf.int,
wt.DI$conf.int[1])),
"CI hi" = as.numeric(c(
wt.OP$conf.int[2],
wt.TP$conf.int[2],
wt.WI$conf.int[2],
wt.DI$conf.int[2])),
p = as.numeric(c(
wt.OP$p.value,
#wt.OP$p.value,
wt.TP$p.value,
#wt.TP$p.value,
wt.WI$p.value,
#wt.WI$p.value,
#wt.DI$p.value,
wt.DI$p.value)))
WA.wilcox
ggplot(WA.wilcox) +
geom_col(aes(test,mean), fill = "grey", color = "black") +
geom_hline(yintercept = 0, color = "black", linetype="dashed") +
scale_x_discrete(name = NULL) +
scale_y_continuous(name = "within - across similarity (Pearson's r)") +
theme_classic() +
theme(text = element_text(family = "LM Roman 10", face="bold")) +
ggsave('fig_similarity.png', width = w, height = h)
ggplot(WA.wilcox) +
geom_col(aes(test,mean), fill = "grey", color = "black") +
geom_hline(yintercept = 0, color = "black", linetype="dashed") +
scale_x_discrete(name = NULL) +
scale_y_continuous(name = "within - across similarity (Pearson's r)") +
theme_classic() +
theme(text = element_text(family = "LM Roman 10", face="bold")) +
ggsave('fig_similarity.png', width = 7, height = 5)
fig_path <- 'C:/Users/Ava/Desktop/Experiments/statistical_learning/3_results/combined'
ggplot(WA.wilcox) +
geom_col(aes(test,mean), fill = "grey", color = "black") +
geom_hline(yintercept = 0, color = "black", linetype="dashed") +
scale_x_discrete(name = NULL) +
scale_y_continuous(name = "within - across similarity (Pearson's r)") +
theme_classic() +
theme(text = element_text(family = "LM Roman 10", face="bold")) +
ggsave(file.path(fig_path,'/fig_similarity.png'), width = 7, height = 5)
# Data Path
data_path <- 'C:/Users/Ava/Desktop/Experiments/statistical_learning/1_data/exp_3'
# Code Path
code_path <- 'C:/Users/Ava/Desktop/Experiments/statistical_learning/2_code/exp_3'
# Results Path
res_path <- 'C:/Users/Ava/Desktop/Experiments/statistical_learning/3_results/exp_3'
fig_path <- file.path(res_path,'/figures') # but can also be changed to : 'C:/Users/Ava/Desktop/Experiments/statistical_learning/3_results/final'
# basics
library(tidyverse)
library(fitdistrplus)
source('C:/Users/Ava/Desktop/Experiments/statistical_learning/2_code/summarySE.R')
source('C:/Users/Ava/Desktop/Experiments/statistical_learning/2_code/R_rainclouds.R')
# stats
library(lme4)
library(emmeans)
library(car)
# save tables
library(stargazer)
# advanced plotting & stats
library(lsr)
library(pwr)
library(RcppRoll) # for rolling means
# latex font
library(extrafont)
loadfonts(device = "win")
par(family="LM Roman 10")
# colors
library("wesanderson")
# default figure size
w = 7
h = 5
theme_set(
theme_classic(base_size = 12)
)
data_path
rt_data_orig <- read_csv(file.path(data_path,'RT_response_table.csv'))
view(rt_data_orig)
rt_data_orig$subject
rm_subjs <- c("b1965", # Did not respond at all
"b5269","l3385", "n0178", # Trials were not blocked, so block numbers meaningless.
"m8718", # Blocking messed up and didn't see all sylls; Block 6 = Target Positions 3,2,3; Blocks seen =  1-16, 1-8
"b7514")
rt_data_orig$subject[rt_data_orig$subjID %in% rm_subjs]
unique(rt_data_orig$subject[rt_data_orig$subjID %in% rm_subjs])
rm_subj_ID_nums <- unique(rt_data_orig$subject[rt_data_orig$subjID %in% rm_subjs])
rm_subj_ID_nums
rt_data_FP <- read_csv(file.path(data_path,'RT_block_summary_table.csv'))
rt_data_FP
rt_data_FP$subject %in% rm_subj_ID_nums
!rt_data_FP$subject %in% rm_subj_ID_nums
unique(rt_data_FP$subject[!rt_data_FP$subject %in% rm_subj_ID_nums])
rm_subj_ID_nums
rt_data_FP <- read_csv(file.path(data_path,'RT_block_summary_table.csv')) %>%
filter(subject == unique(rt_data_FP$subject[!rt_data_FP$subject %in% rm_subj_ID_nums]))
unique(rt_data_FP$subject)
rt_data_FP
sum(rt_data_FP$false_positives)
rt_data_FP
sum(rt_data_FP$hits)/(sum(rt_data_FP$hits)+sum(rt_data_FP$false_positives))
sum(rt_data_FP$hits)/sum(rt_data_FP$targets)
sd(rt_data_FP$hits)
rt_data_orig <- read_csv(file.path(data_path,'RT_response_table.csv'))
# Add blocks
rt_data_clean <- rt_data_orig[which(rt_data_orig$code!=0),]
n_meta_blocks <- max(rt_data_orig$block)/max(rt_data_orig$tgt_pos)
meta_blocks <- array(rep(0,each=nrow(rt_data_clean)))
meta_blocks[which(rt_data_clean$block %in% c(1,2,3))] <- 1
meta_blocks[which(rt_data_clean$block %in% c(4,5,6))] <- 2
meta_blocks[which(rt_data_clean$block %in% c(7,8,9))] <- 3
meta_blocks[which(rt_data_clean$block %in% c(10,11,12))] <- 4
meta_blocks[which(rt_data_clean$block %in% c(13,14,15))] <- 5
meta_blocks[which(rt_data_clean$block %in% c(16,17,18))] <- 6
meta_blocks[which(rt_data_clean$block %in% c(19,20,21))] <- 7
meta_blocks[which(rt_data_clean$block %in% c(22,23,24))] <- 8
rt_data_clean <- add_column(rt_data_clean, meta_blocks, .after = 2)
# Rename & factorize columns
rt_data_cleaned.1 <- rt_data_clean %>%
dplyr::select(2:6,8:11) %>%
dplyr::rename(subject = subjID, trial = block, block = meta_blocks, target_num = code, target = target_syll) %>%
mutate(subject = as.factor(subject),
block = as.factor(block),
target = as.factor(target),
tgt_pos = as.factor(tgt_pos),
tgt_word = as.factor(tgt_word),
# detect = case_when(detect==1 ~ 0,
#           detect==2 ~ 1),
detect = as.numeric(detect))
rm_subjs <- c("b1965", # Did not respond at all
"b5269","l3385", "n0178", # Trials were not blocked, so block numbers meaningless.
"m8718", # Blocking messed up and didn't see all sylls; Block 6 = Target Positions 3,2,3; Blocks seen =  1-16, 1-8
"b7514") # Blocking technically ok, just didn't see all syllables; Blocks seen = 1-12 x 2
rm_subj_ID_nums <- unique(rt_data_orig$subject[rt_data_orig$subjID %in% rm_subjs])
rt_data_cleaned.2 <- rt_data_cleaned.1[which(!rt_data_cleaned.1$subject %in% rm_subjs),]
print(paste0("Of the original ", length(unique(rt_data_cleaned.1$subject)), " subjects, ", length(unique(rt_data_cleaned.2$subject)), " remain after removing participants with technical errors."))
# Outlier Removal Method: median +- 3(mad)
# --> https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/mad
# calculate
mad_norm <- mad(rt_data_cleaned.2$rt,na.rm=TRUE)
upper_bound_med <- median(rt_data_cleaned.2$rt,na.rm=TRUE)+(3*mad_norm)
lower_bound_med <- median(rt_data_cleaned.2$rt,na.rm=TRUE)-(3*mad_norm)
# filter & add scaled variables
rt_data <- rt_data_cleaned.2 %>%
dplyr::filter(rt > lower_bound_med | is.na(rt), # keeps NAs, because these are miss markers
rt < upper_bound_med | is.na(rt)) %>%
mutate(rt_secs = rt/1000,
rt_sc = scale(rt))
# replot
rts.rm <- rt_data$rt %>% na.omit() %>% as.numeric()
hist(rts.rm)
descdist(rts.rm, discrete = FALSE)
fit.norm <- fitdist(rts.rm, "norm")
fit.gam  <- fitdist(rts.rm, "gamma")
fit.ln <- fitdist(rts.rm, "lnorm")
summary(fit.norm)
summary(fit.gam)
summary(fit.ln)
par(mfrow=c(2,2))
plot.legend <- c("norm","gamma", "lognorm")
denscomp(list(fit.norm,fit.gam, fit.ln), legendtext = plot.legend)
cdfcomp (list(fit.norm,fit.gam, fit.ln), legendtext = plot.legend)
qqcomp  (list(fit.norm,fit.gam, fit.ln), legendtext = plot.legend)
ppcomp  (list(fit.norm, fit.gam, fit.ln), legendtext = plot.legend)
gofstat(list(fit.norm, fit.gam, fit.ln), fitnames = c("norm","gamma", "lognorm"))
print(paste0("The method of removing +- 3 * the median absolute deviation results in a data loss of only ", round(1-length(rt_data$rt)/length(rt_data_cleaned.2$rt), digits = 3), "%. Meanwhile, although outlier removal makes the distribution much more normal (note scaling does not add any further normality), gamma remains the best fit."))
rt_data_cleaned.2 %>%
summarise(mean = mean(detect),
sd = sd(detect))
sum(rt_data_FP$hits)/sum(rt_data_FP$targets)
sum(rt_data_FP$hits)/(sum(rt_data_FP$hits)+sum(rt_data_FP$false_positives)) # sensitivity/TPR
sum(rt_data_FP$hits)/(sum(rt_data_FP$hits)+sum(rt_data_FP$false_positives)) # sensitivity/TPR
