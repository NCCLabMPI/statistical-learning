mutate(subject = as.factor(subject),
target = as.factor(target),
tgt_pos = as.factor(tgt_pos),
tgt_word = as.factor(tgt_word))
exp_4_data <- read_csv("C:/Users/Ava/Desktop/Experiments/statistical_learning/1_data/exp_4/exp_4_rt_data.csv") %>%
select(1:2,5:10,12,14) %>%
mutate(subject = as.factor(subject),
target = as.factor(target),
sess = as.factor(sess),
tgt_pos = as.factor(tgt_pos),
tgt_word = as.factor(tgt_word),
cond_order = as.factor(cond_order),
rt_secs = rt/1000) %>%
select(subject,cond_order,sess,trial:target_num,target:rt,rt_secs)
exp_4_data_comb <- exp_4_data_s %>%
mutate(subject = as.character(subject)) %>%
mutate(subject = ifelse(subject == "s1911", "s1912", subject))
exp_4_data_delta_s <- dplyr::filter(exp_4_data_delta,sess=="struct")
exp_4_data_s <- dplyr::filter(exp_4_data,sess=="struct")
exp_4_data_comb <- exp_4_data_s %>%
mutate(subject = as.character(subject)) %>%
mutate(subject = ifelse(subject == "s1911", "s1912", subject))
exps.3.4.data <- rbind(exp_3_data %>% mutate(exp="exp 3") %>%
add_column(cond_order = NA, .after = "subject") %>%
add_column(sess = NA, .before = "block") %>%
dplyr::select(-detect, -rt_sc),
exp_4_data_comb %>% mutate(exp="exp 4") %>%
add_column(block = NA, .before = "trial")) %>%
mutate(target = factor(target, levels = c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")))
# N = 53
exps.3.4.data
exps.3.4.data
write.csv(exps.3.4.data,'C:/Users/Ava/Desktop/Experiments/statistical_learning/1_data/combined/exps_3_4s_data.csv', row.names = FALSE)
# For each subject, compute a correlation matrix between syllables and then z transform that data. Put matrices into a list, and average them. Then transform back to rho.
all.list  <- list()
all.cor.list  <- list()
all.cor.z.list  <- list()
all.cor.diss.z.list <- list()
all.cor.diss.list <- list()
for (curr_subj in unique(exps.3.4.data$subject)) {
curr_data <- exps.3.4.data %>%
filter(subject == curr_subj) %>%
dplyr::select(target,rt) %>%
na.omit() %>%
group_by(target) %>%
dplyr::mutate(row_id=1:n()) %>%
ungroup() %>%
spread(key = target, value = rt) %>%
#ungroup(subject)
dplyr::select(-row_id)
# find minimum number of rows, N
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
# exceptions
if(curr_subj == "s1107") { # 3/26
# subject had only 2 entires in tu, so just make the column NA to preserve other data
curr_data <- curr_data %>%
mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2] # select the second lowest length
} else if (curr_subj == "l2805") { # 3/21
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2]
} else if (curr_subj == "h0207") { # 3/11
curr_data <- curr_data %>%
add_column(ro = as.numeric(rep(NA,nrow(curr_data))))
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[2]
} else if (curr_subj == "g0609") { # 3/8
curr_data <- curr_data %>%
mutate(po = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2] # select the second lowest length
# and maybe also ga?
} else if (curr_subj == "b0207") { # 3/2
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
add_column(ro = as.numeric(rep(NA,nrow(curr_data))),.after = "di")
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[3]
} else if (curr_subj == "a0604") { # 3/1
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2]
} else if (curr_subj == "s2205") { # 4/16
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
add_column(se = as.numeric(rep(NA,nrow(curr_data))),.after = "ki") %>%
mutate(za = as.numeric(rep(NA,nrow(curr_data)))) %>%
mutate(ga = as.numeric(rep(NA,nrow(curr_data))))
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[5]
} else if (curr_subj == "s0310") { # 4/12
# subject is missing be data, so just make it NA, N is the same
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za")
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[2]
} else if (curr_subj == "a2605") { # 4/2
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data)))) %>%
mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[3]
} else {
N <- min(n_rows)
}
# create output matrix with N rows
curr_data.mat <- curr_data[1:N,]
# shuffle longer columns into shorter new matrix
for (i in 1:length(curr_data.mat)) {
if (n_rows[i] < N) {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:N,i]), size = N)
} else {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:n_rows[i],i]), size = N)}
}
# take correlation(), then transform to z values()
curr_data.cor <- cor(curr_data.mat) # corr
curr_data.cor.z <- FisherZ(cor(curr_data.mat)) # corr - fisher
curr_data.cor.diss.z <- FisherZ(1-cor(curr_data.mat)) # diss - fisher
curr_data.cor.diss <- 1-(cor(curr_data.mat)) # diss
# append
all.list <- append(all.list, list(curr_data.mat))
all.cor.list <- append(all.cor.list, list(curr_data.cor))
all.cor.z.list <- append(all.cor.z.list, list(curr_data.cor.z))
all.cor.diss.z.list <- append(all.cor.diss.z.list, list(curr_data.cor.diss.z))
all.cor.diss.list <- append(all.cor.diss, list(curr_data.cor.diss))
}
library(cluster) # clustering algorithms
library(factoextra) # ibid & visualization
library(wesanderson)
pal <- wes_palette("Zissou1", 100, type = "continuous")
library(DescTools) # for FisherZ()
library(ggfortify) # for autoplot in PCA plotting
# For each subject, compute a correlation matrix between syllables and then z transform that data. Put matrices into a list, and average them. Then transform back to rho.
all.list  <- list()
all.cor.list  <- list()
all.cor.z.list  <- list()
all.cor.diss.z.list <- list()
all.cor.diss.list <- list()
for (curr_subj in unique(exps.3.4.data$subject)) {
curr_data <- exps.3.4.data %>%
filter(subject == curr_subj) %>%
dplyr::select(target,rt) %>%
na.omit() %>%
group_by(target) %>%
dplyr::mutate(row_id=1:n()) %>%
ungroup() %>%
spread(key = target, value = rt) %>%
#ungroup(subject)
dplyr::select(-row_id)
# find minimum number of rows, N
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
# exceptions
if(curr_subj == "s1107") { # 3/26
# subject had only 2 entires in tu, so just make the column NA to preserve other data
curr_data <- curr_data %>%
mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2] # select the second lowest length
} else if (curr_subj == "l2805") { # 3/21
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2]
} else if (curr_subj == "h0207") { # 3/11
curr_data <- curr_data %>%
add_column(ro = as.numeric(rep(NA,nrow(curr_data))))
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[2]
} else if (curr_subj == "g0609") { # 3/8
curr_data <- curr_data %>%
mutate(po = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2] # select the second lowest length
# and maybe also ga?
} else if (curr_subj == "b0207") { # 3/2
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
add_column(ro = as.numeric(rep(NA,nrow(curr_data))),.after = "di")
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[3]
} else if (curr_subj == "a0604") { # 3/1
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2]
} else if (curr_subj == "s2205") { # 4/16
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
add_column(se = as.numeric(rep(NA,nrow(curr_data))),.after = "ki") %>%
mutate(za = as.numeric(rep(NA,nrow(curr_data)))) %>%
mutate(ga = as.numeric(rep(NA,nrow(curr_data))))
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[5]
} else if (curr_subj == "s0310") { # 4/12
# subject is missing be data, so just make it NA, N is the same
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za")
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[2]
} else if (curr_subj == "a2605") { # 4/2
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data)))) %>%
mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[3]
} else {
N <- min(n_rows)
}
# create output matrix with N rows
curr_data.mat <- curr_data[1:N,]
# shuffle longer columns into shorter new matrix
for (i in 1:length(curr_data.mat)) {
if (n_rows[i] < N) {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:N,i]), size = N)
} else {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:n_rows[i],i]), size = N)}
}
# take correlation(), then transform to z values()
curr_data.cor <- cor(curr_data.mat) # corr
curr_data.cor.z <- FisherZ(cor(curr_data.mat)) # corr - fisher
curr_data.cor.diss.z <- FisherZ(1-cor(curr_data.mat)) # diss - fisher
curr_data.cor.diss <- 1-(cor(curr_data.mat)) # diss
# append
all.list <- append(all.list, list(curr_data.mat))
all.cor.list <- append(all.cor.list, list(curr_data.cor))
all.cor.z.list <- append(all.cor.z.list, list(curr_data.cor.z))
all.cor.diss.z.list <- append(all.cor.diss.z.list, list(curr_data.cor.diss.z))
all.cor.diss.list <- append(all.cor.diss, list(curr_data.cor.diss))
}
# For each subject, compute a correlation matrix between syllables and then z transform that data. Put matrices into a list, and average them. Then transform back to rho.
all.list  <- list()
all.cor.list  <- list()
all.cor.z.list  <- list()
all.cor.diss.z.list <- list()
all.cor.diss.list <- list()
for (curr_subj in unique(exps.3.4.data$subject)) {
curr_data <- exps.3.4.data %>%
filter(subject == curr_subj) %>%
dplyr::select(target,rt) %>%
na.omit() %>%
group_by(target) %>%
dplyr::mutate(row_id=1:n()) %>%
ungroup() %>%
spread(key = target, value = rt) %>%
#ungroup(subject)
dplyr::select(-row_id)
# find minimum number of rows, N
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
# exceptions
if(curr_subj == "s1107") { # 3/26
# subject had only 2 entires in tu, so just make the column NA to preserve other data
curr_data <- curr_data %>%
mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2] # select the second lowest length
} else if (curr_subj == "l2805") { # 3/21
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2]
} else if (curr_subj == "h0207") { # 3/11
curr_data <- curr_data %>%
add_column(ro = as.numeric(rep(NA,nrow(curr_data))))
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[2]
} else if (curr_subj == "g0609") { # 3/8
curr_data <- curr_data %>%
mutate(po = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2] # select the second lowest length
# and maybe also ga?
} else if (curr_subj == "b0207") { # 3/2
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
add_column(ro = as.numeric(rep(NA,nrow(curr_data))),.after = "di")
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[3]
} else if (curr_subj == "a0604") { # 3/1
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[2]
} else if (curr_subj == "s2205") { # 4/16
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
add_column(se = as.numeric(rep(NA,nrow(curr_data))),.after = "ki") %>%
mutate(za = as.numeric(rep(NA,nrow(curr_data)))) %>%
mutate(ga = as.numeric(rep(NA,nrow(curr_data))))
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[5]
} else if (curr_subj == "s0310") { # 4/12
# subject is missing be data, so just make it NA, N is the same
curr_data <- curr_data %>%
add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za")
n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
N <- sort(n_rows)[2]
} else if (curr_subj == "a2605") { # 4/2
curr_data <- curr_data %>%
mutate(be = as.numeric(rep(NA,nrow(curr_data)))) %>%
mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
N <- sort(n_rows)[3]
} else {
N <- min(n_rows)
}
# create output matrix with N rows
curr_data.mat <- curr_data[1:N,]
# shuffle longer columns into shorter new matrix
for (i in 1:length(curr_data.mat)) {
if (n_rows[i] < N) {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:N,i]), size = N)
} else {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:n_rows[i],i]), size = N)}
}
# take correlation(), then transform to z values()
curr_data.cor <- cor(curr_data.mat) # corr
curr_data.cor.z <- FisherZ(cor(curr_data.mat)) # corr - fisher
curr_data.cor.diss.z <- FisherZ(1-cor(curr_data.mat)) # diss - fisher
curr_data.cor.diss <- 1-(cor(curr_data.mat)) # diss
# append
all.list <- append(all.list, list(curr_data.mat))
all.cor.list <- append(all.cor.list, list(curr_data.cor))
all.cor.z.list <- append(all.cor.z.list, list(curr_data.cor.z))
all.cor.diss.z.list <- append(all.cor.diss.z.list, list(curr_data.cor.diss.z))
all.cor.diss.list <- append(all.cor.diss.list, list(curr_data.cor.diss))
}
# avg.z.list <- reduce(all.z.list, `+`, na.rm = TRUE) / length(all.z.list)
# ^ can't handle NAs
# avg.cor.list <- rowMeans(do.call(cbind, all.cor.list), na.rm = TRUE)
# ^ doesn't perform intended function (mean matrix), but also, the Inf values generated
# by the Fisher transform make the outputs Inf. Possible solution is to recode z of Inf
# for cor of 1 to the z value for a correlation of 0.99...
# But I wonder if it's necesary to perform the z transform. Why not just average the correlation
# matrices? After all, they are all on the same scale and express relationships between the items,
# rather than absolute values, which is what we intended.
# So now, just find how to perform matrix average that ignores NAs in the correlation matrix.
# Fisher Z merely makes the sampling distribution more normal to make calculation easier, but
# it's not required if the computation is performed on a computer that can handle the exact
# (however skewed) true distribution: https://stats.stackexchange.com/questions/420142/why-is-fisher-transformation-necessary
# Ok, these work! For the z's, the diag is infinite
# all.cor <- apply(simplify2array(all.cor.list), 1:2, mean, na.rm= TRUE)
# all.z <- apply(simplify2array(all.cor.z.list), 1:2, mean, na.rm= TRUE)
# The results of these two are nearly identical -- identical out to about 3 digits, but indeed the z's are normal-ish
# all.diss.z <- FisherZ(all.cor.diss)
# apply(simplify2array(all.cor.diss),1:2, na.rm = TRUE)
# indices for each matrix
# wn.TP <- list(c(1,4),c(1,7),c(1,10),c(4,7),c(4,10),c(7,10), # low TP
#               c(2,5),c(2,8),c(2,11),c(5,8),c(5,11),c(8,11), # high TP
#              c(3,6),c(3,9),c(3,12),c(6,9),c(6,12),c(9,12))
# ac.TP <- list(c(1,2),c(4,5),c(7,8),c(10,11)) # .33-1 crosses within words
wn.TP <- list(c(1,4),c(1,7),c(1,10),c(4,7),c(4,10),c(7,10)) # low TP
ac.TP <- list(c(2,5),c(2,8),c(2,11),c(5,8),c(5,11),c(8,11), # high TP
c(3,6),c(3,9),c(3,12),c(6,9),c(6,12),c(9,12))
wn.tp.arr <- list()
for (i in 1:length(wn.TP)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
wn.tp.arr <- append(wn.tp.arr, mapply(function(x, y) x[y][2], all.cor.list, wn.TP[i], SIMPLIFY = FALSE))
}
ac.tp.arr <- list()
for (i in 1:length(ac.TP)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
ac.tp.arr <- append(ac.tp.arr, mapply(function(x, y) x[y][2], all.cor.list, ac.TP[i], SIMPLIFY = FALSE))
}
within.TP <- cbind(wn.tp.arr[!is.na(wn.tp.arr)]) %>% as.numeric()
across.TP <- cbind(ac.tp.arr[!is.na(ac.tp.arr)]) %>% as.numeric()
n.tp <- round(min(length(within.TP),length(across.TP))*(4/5))
set.seed(42)
within.TP.samp <- replicate(200, sample(within.TP, size = n.tp, replace = TRUE))
within.TP.means <- replicate(200, mean(sample(within.TP, size = n.tp, replace = TRUE)))
across.TP.samp <- replicate(200, sample(across.TP, size = n.tp, replace = TRUE))
across.TP.means <- replicate(200, mean(sample(across.TP, size = n.tp, replace = TRUE)))
(wt.TP <- wilcox.test(within.TP.samp,across.TP.samp, conf.int = TRUE, conf.level = 0.95, paired = FALSE))
# indices for each matrix
wn.OP <- list(c(1,4),c(1,7),c(1,10),c(4,7),c(4,10),c(7,10), # 1s
c(2,5),c(2,8),c(2,11),c(5,8),c(5,11),c(8,11), #2s
c(3,6),c(3,9),c(3,12),c(6,9),c(6,12),c(9,12)) #3s
ac.OP <- list(c(1,2),c(2,3),c(1,3),
c(4,5),c(5,6),c(4,6),
c(7,8),c(7,9),c(8,9),
c(10,11),c(11,12),c(10,12))
# collect all possible values
wn.arr <- list()
for (i in 1:length(wn.OP)) {
wn.arr <- append(wn.arr, mapply(function(x, y) x[y][2], all.cor.list, wn.OP[i], SIMPLIFY = FALSE))
}
ac.arr <- list()
for (i in 1:length(ac.OP)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
ac.arr <- append(ac.arr, mapply(function(x, y) x[y][2], all.cor.list, ac.OP[i], SIMPLIFY = FALSE))
}
within.OP <- cbind(wn.arr[!is.na(wn.arr)]) %>% as.numeric()
across.OP <- cbind(ac.arr[!is.na(ac.arr)]) %>% as.numeric()
# compute maximum length
n.op <- round(min(length(within.OP),length(across.OP))*(4/5))
# sample & bootstrap to produce a matrix and bootstrapped means
set.seed(42)
within.OP.samp <- replicate(200, sample(within.OP, size = n.op, replace = TRUE))
within.OP.means <- replicate(200, mean(sample(within.OP, size = n.op, replace = TRUE)))
across.OP.samp <- replicate(200, sample(across.OP, size = n.op, replace = TRUE))
across.OP.means <- replicate(200, mean(sample(across.OP, size = n.op, replace = TRUE)))
(wt.OP <- wilcox.test(within.OP.samp,across.OP.samp, conf.int = TRUE, conf.level = 0.95, paired = FALSE))
wn.WI <- list(c(1,2),c(2,3),c(1,3),# nugadi
c(4,5),c(5,6),c(4,6),# rokise
c(7,8),c(7,9),c(8,9),# mipola
c(10,11),c(11,12),c(10,12))# zabetu
ac.WI <- list(c(1,5),c(2,6),c(1,6),c(1,8),c(2,9),c(1,9),c(1,11),c(2,12),c(1,12),# nu-ki, ga-se, nu-se, nu-po, ga-la, nu-la, nu-be, ga-tu, nu-tu
c(4,2),c(5,3),c(4,3),c(4,8),c(5,9),c(4,9),c(4,11),c(5,12),c(4,12), # ro-ga, ki-di, ro-di, ro-po, ki-la, ro-la, ro-be, ki-tu, ro-tu
c(7,2),c(8,3),c(7,3),c(7,5),c(8,6),c(7,6),c(7,11),c(8,12),c(7,12), # mi-ga, po-di, mi-di, mi-ki, po-se, mi-se, mi-be, po-tu, mi-tu
c(10,2),c(11,3),c(10,3),c(11,5),c(10,6),c(11,6),c(10,8),c(11,9),c(10,9)) # za-ga, be-di, za-di, za-ki, be-ki, za-se, za-po, be-la, za-la
# nope, these are all 3-1s
# list(c(3,4),c(3,7),c(3,10), #di-ro, di-mi, di-za
#             c(6,1),c(6,7),c(6,10), #se-nu, se-mi, se-za
#             c(9,1),c(9,4),c(9,10), #la-nu, la-ro, la-za
#             c(12,1),c(12,4,c(12,7))) #za-nu, za-ro, za-mi
wn.WI.arr <- list()
for (i in 1:length(wn.WI)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
wn.WI.arr <- append(wn.WI.arr, mapply(function(x, y) x[y][2], all.cor.list, wn.WI[i], SIMPLIFY = FALSE))
}
ac.WI.arr <- list()
for (i in 1:length(ac.WI)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
ac.WI.arr <- append(ac.WI.arr, mapply(function(x, y) x[y][2], all.cor.list, ac.WI[i], SIMPLIFY = FALSE))
}
within.WI <- cbind(wn.WI.arr[!is.na(wn.WI.arr)]) %>% as.numeric()
across.WI <- cbind(ac.WI.arr[!is.na(ac.WI.arr)]) %>% as.numeric()
n.wi <- round(min(length(within.WI),length(across.WI))*(4/5))
set.seed(42)
within.WI.samp <- replicate(200, sample(within.WI, size = n.wi, replace = TRUE))
within.WI.means <- replicate(200, mean(sample(within.WI, size = n.wi, replace = TRUE)))
across.WI.samp <- replicate(200, sample(across.WI, size = n.wi, replace = TRUE))
across.WI.means <- replicate(200, mean(sample(across.WI, size = n.wi, replace = TRUE)))
(wt.WI <- wilcox.test(within.WI.samp,across.WI.samp, conf.int = TRUE, conf.level = 0.95, paired = FALSE))
wn.di <- list(c(1,2),c(2,3),
c(4,5),c(5,6),
c(7,8),c(7,9),
c(10,11),c(11,12))
ac.di <- list(c(1,3),c(4,6),c(8,9),c(10,12))
wn.di.arr <- list()
for (i in 1:length(wn.di)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
wn.di.arr <- append(wn.di.arr, mapply(function(x, y) x[y][2], all.cor.list, wn.di[i], SIMPLIFY = FALSE))
}
ac.di.arr <- list()
for (i in 1:length(ac.di)) {
# all.cor.list[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
ac.di.arr <- append(ac.di.arr, mapply(function(x, y) x[y][2], all.cor.list, ac.di[i], SIMPLIFY = FALSE))
}
within.di <- cbind(wn.di.arr[!is.na(wn.di.arr)]) %>% as.numeric()
across.di <- cbind(ac.di.arr[!is.na(ac.di.arr)]) %>% as.numeric()
n.di <- round(min(length(within.di),length(across.di))*(4/5))
set.seed(42)
within.di.samp <- replicate(200, sample(within.di, size = n.di, replace = TRUE))
within.di.means <- replicate(200, mean(sample(within.di, size = n.di, replace = TRUE)))
across.di.samp <- replicate(200, sample(across.di, size = n.di, replace = TRUE))
across.di.means <- replicate(200, mean(sample(across.di, size = n.di, replace = TRUE)))
(wt.DI <- wilcox.test(within.di.samp,across.di.samp, conf.int = TRUE, conf.level = 0.95, paired = FALSE))
WA.wilcox <- data.frame(test = factor(c(
"ordinal position",
#"ordinal position",
"transitional probability",
#"transitional probability",
"word identity",
#"word identity",
#"duplets",
"duplets"), levels = c("ordinal position","transitional probability","word identity","duplets")),
#group = as.factor(c("within","across","within","across","within","across","within","across")),
mean = as.numeric(c(mean(within.OP.means)-mean(across.OP.means),
mean(within.TP.means)-mean(across.TP.means),
mean(within.WI.means)-mean(across.WI.means),
mean(within.di.means)-mean(across.di.means))),
w = as.numeric(c(
wt.OP$statistic,
#wt.OP$statistic,
wt.TP$statistic,
#wt.TP$statistic,
wt.WI$statistic,
#wt.WI$statistic,
#wt.DI$statistic,
wt.DI$statistic)),
"CI low" = as.numeric(c(
wt.OP$conf.int[1],
#wt.OP$conf.int,
wt.TP$conf.int[1],
#wt.TP$conf.int,
wt.WI$conf.int[1],
#wt.WI$conf.int,
#wt.DI$conf.int,
wt.DI$conf.int[1])),
"CI hi" = as.numeric(c(
wt.OP$conf.int[2],
wt.TP$conf.int[2],
wt.WI$conf.int[2],
wt.DI$conf.int[2])),
p = as.numeric(c(
wt.OP$p.value,
#wt.OP$p.value,
wt.TP$p.value,
#wt.TP$p.value,
wt.WI$p.value,
#wt.WI$p.value,
#wt.DI$p.value,
wt.DI$p.value)))
WA.wilcox
ggplot(WA.wilcox) +
geom_col(aes(test,mean), fill = "grey", color = "black") +
geom_hline(yintercept = 0, color = "black", linetype="dashed") +
scale_x_discrete(name = NULL) +
scale_y_continuous(name = "within - across similarity (Pearson's r)") +
theme_classic() +
theme(text = element_text(family = "LM Roman 10", face="bold")) +
ggsave('fig_similarity.png', width = w, height = h)
ggplot(WA.wilcox) +
geom_col(aes(test,mean), fill = "grey", color = "black") +
geom_hline(yintercept = 0, color = "black", linetype="dashed") +
scale_x_discrete(name = NULL) +
scale_y_continuous(name = "within - across similarity (Pearson's r)") +
theme_classic() +
theme(text = element_text(family = "LM Roman 10", face="bold")) +
ggsave('fig_similarity.png', width = 7, height = 5)
fig_path <- 'C:/Users/Ava/Desktop/Experiments/statistical_learning/3_results/combined'
ggplot(WA.wilcox) +
geom_col(aes(test,mean), fill = "grey", color = "black") +
geom_hline(yintercept = 0, color = "black", linetype="dashed") +
scale_x_discrete(name = NULL) +
scale_y_continuous(name = "within - across similarity (Pearson's r)") +
theme_classic() +
theme(text = element_text(family = "LM Roman 10", face="bold")) +
ggsave(file.path(fig_path,'/fig_similarity.png'), width = 7, height = 5)
